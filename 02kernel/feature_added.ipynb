{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jk/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Initially forked from Bojan's kernel here: https://www.kaggle.com/tunguz/bow-meta-text-and-dense-features-lb-0-2242/code\n",
    "#improvement using kernel from Nick Brook's kernel here: https://www.kaggle.com/nicapotato/bow-meta-text-and-dense-features-lgbm\n",
    "#Used oof method from Faron's kernel here: https://www.kaggle.com/mmueller/stacking-starter?scriptVersionId=390867\n",
    "#Used some text cleaning method from Muhammad Alfiansyah's kernel here: https://www.kaggle.com/muhammadalfiansyah/push-the-lgbm-v19\n",
    "#Forked From - https://www.kaggle.com/him4318/avito-lightgbm-with-ridge-feature-v-2-0\n",
    "\n",
    "#Time features - Referrenced from Benjamin's kernal from https://www.kaggle.com/bminixhofer/aggregated-features-lightgbm\n",
    "\n",
    "import time\n",
    "notebookstart= time.time()\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "random.seed(2018)\n",
    "# print(\"Data:\\n\",os.listdir(\"../input\"))\n",
    "\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "# Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Tf-Idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS = 10\n",
    "SEED = 2018\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None, seed_bool = True):\n",
    "        if(seed_bool == True):\n",
    "            params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "        \n",
    "\n",
    "def get_oof(clf, x_train, y, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        print('\\nFold {}'.format(i))\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "def cleanName(text):\n",
    "    try:\n",
    "        textProc = text.lower()\n",
    "        # textProc = \" \".join(map(str.strip, re.split('(\\d+)',textProc)))\n",
    "        #regex = re.compile(u'[^[:alpha:]]')\n",
    "        #textProc = regex.sub(\" \", textProc)\n",
    "        textProc = re.sub('[!@#$_“”¨«»®´·º½¾¿¡§£₤‘’]', '', textProc)\n",
    "        textProc = \" \".join(textProc.split())\n",
    "        return textProc\n",
    "    except: \n",
    "        return \"name error\"\n",
    "    \n",
    "    \n",
    "def rmse(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power((y - y0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Load Stage\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Load Stage\")\n",
    "training = pd.read_csv('../train.csv')\n",
    "traindex = training.index\n",
    "testing = pd.read_csv('../test.csv')\n",
    "testdex = testing.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_blur = pd.read_csv(\"../train_blurrness.csv\")\n",
    "test_blur = pd.read_csv(\"../test_blurrness.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging image blurrness score\n"
     ]
    }
   ],
   "source": [
    "print (\"Merging image blurrness score\")\n",
    "training = training.merge(train_blur, on='item_id', how='left')\n",
    "testing = testing.merge(test_blur, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: 1503424 Rows, 18 Columns\n",
      "Test shape: 508438 Rows, 18 Columns\n",
      "Combine Train and Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2471"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntrain = training.shape[0]\n",
    "ntest = testing.shape[0]\n",
    "\n",
    "kf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "y = training.deal_probability.copy()\n",
    "training.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*training.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testing.shape))\n",
    "\n",
    "print(\"Combine Train and Test\")\n",
    "df = pd.concat([training,testing],axis=0)\n",
    "del training, testing\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Data shape: 2011862 Rows, 18 Columns\n",
      "Adding date features - Load files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))\n",
    "###\n",
    "print (\"Adding date features - Load files\")\n",
    "train = pd.read_csv(\"../train.csv\", usecols=['item_id', 'user_id'])\n",
    "test = pd.read_csv(\"../test.csv\", usecols=['item_id', 'user_id'])\n",
    "\n",
    "train_active = pd.read_csv(\"../train_active.csv\", usecols = ['item_id', 'user_id'])\n",
    "test_active = pd.read_csv(\"../test_active.csv\", usecols = ['item_id', 'user_id'])\n",
    "\n",
    "train_periods = pd.read_csv('../periods_train.csv', parse_dates=['date_from', 'date_to'])\n",
    "test_periods = pd.read_csv('../periods_test.csv', parse_dates=['date_from', 'date_to'])\n",
    "\n",
    "agg = pd.concat([train, test, train_active, test_active]).reset_index(drop=True)\n",
    "agg.drop_duplicates(['item_id'], inplace=True)\n",
    "\n",
    "del train_active, test_active\n",
    "gc.collect()\n",
    "\n",
    "period = pd.concat([train_periods, test_periods])\n",
    "del train_periods, test_periods\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create features\n",
      "Merge on df\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Create features\")\n",
    "period[\"days_up\"] = period['date_to'].dt.dayofyear - period['date_from'].dt.dayofyear\n",
    "\n",
    "gp = period.groupby(['item_id'])[['days_up']]\n",
    "\n",
    "gp_df = pd.DataFrame()\n",
    "gp_df['days_up_sum'] = gp.sum()['days_up']\n",
    "gp_df['times_put_up'] = gp.count()['days_up']\n",
    "gp_df.reset_index(inplace=True)\n",
    "gp_df.rename(index=str, columns={'index': 'item_id'})\n",
    "\n",
    "period.drop_duplicates(['item_id'], inplace=True)\n",
    "period = period.merge(gp_df, on='item_id', how='left')\n",
    "period = period.merge(agg, on='item_id', how='left')\n",
    "\n",
    "gp = period.groupby(['user_id'])[['days_up_sum', 'times_put_up']].mean().reset_index() \\\n",
    "    .rename(index=str, columns={\n",
    "        'days_up_sum': 'avg_days_up_user',\n",
    "        'times_put_up': 'avg_times_up_user'\n",
    "    })\n",
    "\n",
    "n_user_items = agg.groupby(['user_id'])[['item_id']].count().reset_index() \\\n",
    "    .rename(index=str, columns={\n",
    "        'item_id': 'n_user_items'\n",
    "    })\n",
    "\n",
    "gp = gp.merge(n_user_items, on='user_id', how='outer')\n",
    "\n",
    "print (\"Merge on df\")\n",
    "df = df.merge(gp, on = \"user_id\", how = \"left\")\n",
    "\n",
    "del gp, n_user_items, period, agg, gp_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Regional\n",
    "region_map = {\"Свердловская область\" : \"Sverdlovsk oblast\",\n",
    "            \"Самарская область\" : \"Samara oblast\",\n",
    "            \"Ростовская область\" : \"Rostov oblast\",\n",
    "            \"Татарстан\" : \"Tatarstan\",\n",
    "            \"Волгоградская область\" : \"Volgograd oblast\",\n",
    "            \"Нижегородская область\" : \"Nizhny Novgorod oblast\",\n",
    "            \"Пермский край\" : \"Perm Krai\",\n",
    "            \"Оренбургская область\" : \"Orenburg oblast\",\n",
    "            \"Ханты-Мансийский АО\" : \"Khanty-Mansi Autonomous Okrug\",\n",
    "            \"Тюменская область\" : \"Tyumen oblast\",\n",
    "            \"Башкортостан\" : \"Bashkortostan\",\n",
    "            \"Краснодарский край\" : \"Krasnodar Krai\",\n",
    "            \"Новосибирская область\" : \"Novosibirsk oblast\",\n",
    "            \"Омская область\" : \"Omsk oblast\",\n",
    "            \"Белгородская область\" : \"Belgorod oblast\",\n",
    "            \"Челябинская область\" : \"Chelyabinsk oblast\",\n",
    "            \"Воронежская область\" : \"Voronezh oblast\",\n",
    "            \"Кемеровская область\" : \"Kemerovo oblast\",\n",
    "            \"Саратовская область\" : \"Saratov oblast\",\n",
    "            \"Владимирская область\" : \"Vladimir oblast\",\n",
    "            \"Калининградская область\" : \"Kaliningrad oblast\",\n",
    "            \"Красноярский край\" : \"Krasnoyarsk Krai\",\n",
    "            \"Ярославская область\" : \"Yaroslavl oblast\",\n",
    "            \"Удмуртия\" : \"Udmurtia\",\n",
    "            \"Алтайский край\" : \"Altai Krai\",\n",
    "            \"Иркутская область\" : \"Irkutsk oblast\",\n",
    "            \"Ставропольский край\" : \"Stavropol Krai\",\n",
    "            \"Тульская область\" : \"Tula oblast\"}\n",
    "            \n",
    "df['region_en'] = df['region'].apply(lambda x : cleanName(region_map[x]))\n",
    "\n",
    "regional = pd.read_csv(\"../regional.csv\", index_col = [0])\n",
    "regional[\"region_en\"] = regional.index\n",
    "regional[\"region_en\"] = regional[\"region_en\"].apply(lambda x: cleanName(x))\n",
    "\n",
    "df = df.merge(regional, on = \"region_en\", how = \"left\").drop(\"region_en\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2017-03-28\n",
       "1    2017-03-26\n",
       "2    2017-03-20\n",
       "3    2017-03-25\n",
       "4    2017-03-16\n",
       "Name: activation_date, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.activation_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering - Numerical\n",
      "\n",
      "Create Time Variables\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "print(\"Feature Engineering - Numerical\")\n",
    "df[\"price\"] = np.log(df[\"price\"]+0.001)\n",
    "df[\"price\"].fillna(df.price.mean(),inplace=True)\n",
    "df[\"Total_population\"] = np.log(df[\"Total_population\"]+0.001)\n",
    "df[\"Total_population\"].fillna(df.Total_population.mean(),inplace=True)\n",
    "\n",
    "numeric = [\"image_blurrness_score\", \"avg_days_up_user\", \"avg_times_up_user\", \"n_user_items\", \"Density_of_region(km2)\", \"Rural_%\", \"Urban%\"]\n",
    "for col in numeric:\n",
    "    df[col].fillna(-1, inplace = True)\n",
    "\n",
    "df['activation_date'] =  pd.to_datetime(df['activation_date'], format = \"%Y-%m-%d\")\n",
    "\n",
    "print(\"\\nCreate Time Variables\")\n",
    "df[\"Weekday\"] = df['activation_date'].dt.weekday\n",
    "df[\"Weekd of Year\"] = df['activation_date'].dt.week\n",
    "df[\"Day of Month\"] = df['activation_date'].dt.day\n",
    "\n",
    "# Create Validation Index and Remove Dead Variables\n",
    "training_index = df.loc[df.activation_date<=pd.to_datetime('2017-04-07')].index\n",
    "validation_index = df.loc[df.activation_date>=pd.to_datetime('2017-04-08')].index\n",
    "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encode Variables\n",
      "Encoding : ['user_id', 'region', 'city', 'parent_category_name', 'category_name', 'user_type', 'image_top_1', 'param_1', 'param_2', 'param_3', 'Time_zone']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEncode Variables\")\n",
    "categorical = [\"user_id\",\"region\",\"city\",\"parent_category_name\",\"category_name\",\"user_type\",\"image_top_1\",\"param_1\",\"param_2\",\"param_3\", \"Time_zone\"]\n",
    "print(\"Encoding :\",categorical)\n",
    "\n",
    "# Encoder:\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    df[col].fillna('Unknown')\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nText Features\")\n",
    "\n",
    "# Feature Engineering \n",
    "\n",
    "# Meta Text Features\n",
    "textfeats = [\"description\", \"title\"]\n",
    "df['desc_punc'] = df['description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "df['title'] = df['title'].apply(lambda x: cleanName(x))\n",
    "df[\"description\"]   = df[\"description\"].apply(lambda x: cleanName(x))\n",
    "\n",
    "for cols in textfeats:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('missing') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n",
    "    df[cols + '_num_letters'] = df[cols].apply(lambda comment: len(comment)) # Count number of Letters\n",
    "    df[cols + '_num_alphabets'] = df[cols].apply(lambda comment: (comment.count(r'[a-zA-Z]'))) # Count number of Alphabets\n",
    "    df[cols + '_num_alphanumeric'] = df[cols].apply(lambda comment: (comment.count(r'[A-Za-z0-9]'))) # Count number of AlphaNumeric\n",
    "    df[cols + '_num_digits'] = df[cols].apply(lambda comment: (comment.count('[0-9]'))) # Count number of Digits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Feature Engineering\n",
    "df['title_desc_len_ratio'] = df['title_num_letters']/df['description_num_letters']\n",
    "\n",
    "print(\"\\n[TF-IDF] Term Frequency Inverse Document Frequency Stage\")\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"analyzer\": 'word',\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"norm\": 'l2',\n",
    "    #\"min_df\":5,\n",
    "    #\"max_df\":.9,\n",
    "    \"smooth_idf\":False\n",
    "}\n",
    "\n",
    "\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "vectorizer = FeatureUnion([\n",
    "        ('description',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=17000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('description'))),\n",
    "        ('title',CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words = russian_stop,\n",
    "            #max_features=7000,\n",
    "            preprocessor=get_col('title')))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_vect=time.time()\n",
    "\n",
    "#Fit my vectorizer on the entire dataset instead of the training rows\n",
    "\n",
    "vectorizer.fit(df.to_dict('records'))\n",
    "\n",
    "ready_df = vectorizer.transform(df.to_dict('records'))\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "print(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Text Cols\n",
    "textfeats = [\"description\", \"title\"]\n",
    "df.drop(textfeats, axis=1,inplace=True)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "ridge_params = {'alpha':30.0, 'fit_intercept':True, 'normalize':False, 'copy_X':True,\n",
    "                'max_iter':None, 'tol':0.001, 'solver':'auto', 'random_state':SEED}\n",
    "\n",
    "#Ridge oof method from Faron's kernel\n",
    "#I was using this to analyze my vectorization, but figured it would be interesting to add the results back into the dataset\n",
    "#It doesn't really add much to the score, but it does help lightgbm converge faster\n",
    "ridge = SklearnWrapper(clf=Ridge, seed = SEED, params = ridge_params)\n",
    "ridge_oof_train, ridge_oof_test = get_oof(ridge, ready_df[:ntrain], y, ready_df[ntrain:])\n",
    "\n",
    "rms = sqrt(mean_squared_error(y, ridge_oof_train))\n",
    "print('Ridge OOF RMSE: {}'.format(rms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Modeling Stage\")\n",
    "\n",
    "ridge_preds = np.concatenate([ridge_oof_train, ridge_oof_test])\n",
    "\n",
    "df['ridge_preds'] = ridge_preds\n",
    "\n",
    "df = df.set_index('item_id')\n",
    "\n",
    "# Combine Dense Features with Sparse Text Bag of Words Features\n",
    "X = hstack([csr_matrix(df.loc[traindex,:].values),ready_df[0:traindex.shape[0]]]) # Sparse Matrix\n",
    "testing = hstack([csr_matrix(df.loc[testdex,:].values),ready_df[traindex.shape[0]:]])\n",
    "tfvocab = df.columns.tolist() + tfvocab\n",
    "for shape in [X,testing]:\n",
    "    print(\"{} Rows and {} Cols\".format(*shape.shape))\n",
    "print(\"Feature Names Length: \",len(tfvocab))\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModeling Stage\")\n",
    "\n",
    "del vectorizer,ready_df\n",
    "gc.collect();\n",
    "    \n",
    "print(\"Light Gradient Boosting Regressor\")\n",
    "lgbm_params =  {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    #'max_depth': 15,\n",
    "    'num_leaves': 270,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 2,\n",
    "    'learning_rate': 0.0175,\n",
    "    'verbose': 0,\n",
    "    'reg_rambda':0.75,\n",
    "}  \n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=2018)\n",
    "        \n",
    "\n",
    "\n",
    "lgtrain = lgb.Dataset(X_train, y_train,feature_name=tfvocab,categorical_feature = categorical)\n",
    "lgvalid = lgb.Dataset(X_valid, y_valid,feature_name=tfvocab,categorical_feature = categorical)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.train(lgbm_params,lgtrain,num_boost_round=20000,\n",
    "        valid_sets=[lgtrain, lgvalid],\n",
    "        valid_names=['train','valid'],\n",
    "        early_stopping_rounds=500,\n",
    "        verbose_eval=500)\n",
    "\n",
    "# print(\"Model Evaluation Stage\")\n",
    "# print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, lgb_clf.predict(X_valid))))\n",
    "# del X_valid ; gc.collect()\n",
    "\n",
    "\n",
    "# Feature Importance Plot\n",
    "f, ax = plt.subplots(figsize=[7,10])\n",
    "lgb.plot_importance(lgb_clf, max_num_features=100, ax=ax)\n",
    "plt.title(\"Light GBM Feature Importance\")\n",
    "# plt.savefig('feature_import.png')\n",
    "\n",
    "print(\"Put Stage\")\n",
    "lgpred = lgb_clf.predict(testing) \n",
    "\n",
    "#Mixing lightgbm with ridge. I haven't really tested if this improves the score or not\n",
    "#blend = 0.95*lgpred + 0.05*ridge_oof_test[:,0]\n",
    "lgsub = pd.DataFrame(lgpred,columns=[\"deal_probability\"],index=testdex)\n",
    "lgsub['deal_probability'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\n",
    "lgsub.to_csv(\"ridgelgb.csv\",index=True,header=True)\n",
    "#print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n",
    "print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

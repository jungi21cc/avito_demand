{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eb59ba957119aaebc240c48f6d986c993302db74"
   },
   "source": [
    "## Import all the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing, model_selection, metrics\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from datetime import date\n",
    "\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "0156787da5137b8d5c43919607492e296f494fef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../train.csv\", parse_dates=[\"activation_date\"])\n",
    "test_df = pd.read_csv(\"../test.csv\", parse_dates=[\"activation_date\"])\n",
    "print(\"Train file rows and columns are : \", train_df.shape)\n",
    "print(\"Test file rows and columns are : \", test_df.shape)\n",
    "\n",
    "train_prd = pd.read_csv(\"../periods_train.csv\", parse_dates=[\"activation_date\",\"date_from\", \"date_to\"])\n",
    "test_prd = pd.read_csv(\"../periods_test.csv\", parse_dates=[\"activation_date\",\"date_from\", \"date_to\"])\n",
    "print(\"Period Train file rows and columns are : \", train_prd.shape)\n",
    "print(\"Period Test file rows and columns are : \", test_prd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "29844b1e6f08510549cf3ec431a6974280c1ce81"
   },
   "outputs": [],
   "source": [
    "train_prd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "9b702534b16ad97dfca029cf483ce20be40e1e4e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of days an ad was active on the portal\n",
    "train_prd['days'] = (train_prd['date_to'] - train_prd['date_from']).dt.days\n",
    "test_prd['days'] = (test_prd['date_to'] - test_prd['date_from']).dt.days\n",
    "\n",
    "enc = train_prd.groupby('item_id')['days'].agg('sum').astype(np.float32).reset_index()\n",
    "enc.head(5)\n",
    "\n",
    "train_df = pd.merge(train_df, enc, how='left', on='item_id')\n",
    "test_df = pd.merge(test_df, enc, how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "12ab80657fa592a5917faed2bffe6e636c3f0cc0"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1057760c07fee56176267711396b09fbbec543aa"
   },
   "source": [
    "# IMPUTE Missing Values for days active\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c01e6f097c01dbf0db282b7db5103626aeeed03c"
   },
   "source": [
    "## Create new variables and process the existing ones (train.cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "a5fad3bb6c39309f7060158966a55135a93bde48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New variables #\n",
    "train_df[\"activation_weekday\"] = train_df[\"activation_date\"].dt.weekday\n",
    "test_df[\"activation_weekday\"] = test_df[\"activation_date\"].dt.weekday\n",
    "\n",
    "train_df[\"activation_month\"] = train_df[\"activation_date\"].dt.month\n",
    "test_df[\"activation_month\"] = test_df[\"activation_date\"].dt.month\n",
    "\n",
    "train_df[\"title_nwords\"] = train_df[\"title\"].apply(lambda x: len(x.split()))\n",
    "test_df[\"title_nwords\"] = test_df[\"title\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "train_df[\"description\"].fillna(\"NA\", inplace=True)\n",
    "test_df[\"description\"].fillna(\"NA\", inplace=True)\n",
    "train_df[\"desc_nwords\"] = train_df[\"description\"].apply(lambda x: len(x.split()))\n",
    "test_df[\"desc_nwords\"] = test_df[\"description\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "train_df['param123'] = train_df['param_1'].fillna('') + \" \" + train_df['param_2'].fillna('') + \" \" + train_df['param_3'].fillna('') \n",
    "test_df['param123'] = test_df['param_1'].fillna('') + \" \" + test_df['param_2'].fillna('') + \" \" + test_df['param_3'].fillna('') \n",
    "\n",
    "#Impute image_top_1\n",
    "enc = train_df.groupby('category_name')['image_top_1'].agg(lambda x:x.value_counts().index[0]).astype(np.float32).reset_index()\n",
    "enc.columns = ['category_name' ,'image_top_1_impute']\n",
    "#Cross Check values\n",
    "#enc = train_df.loc[train_df['category_name'] == 'Аквариум'].groupby('image_top_1').agg('count')\n",
    "#enc.sort_values(['item_id'], ascending=False).head(2)\n",
    "\n",
    "train_df = pd.merge(train_df, enc, how='left', on='category_name')\n",
    "test_df = pd.merge(test_df, enc, how='left', on='category_name')\n",
    "\n",
    "train_df['image_top_1'].fillna(train_df['image_top_1_impute'], inplace=True)\n",
    "test_df['image_top_1'].fillna(test_df['image_top_1_impute'], inplace=True)\n",
    "\n",
    "#Impute Days diff\n",
    "enc = train_df.groupby('category_name')['days'].agg('median').astype(np.float32).reset_index()\n",
    "enc.columns = ['category_name' ,'days_impute']\n",
    "#Cross Check values\n",
    "#enc = train_df.loc[train_df['category_name'] == 'Аквариум'].groupby('image_top_1').agg('count')\n",
    "#enc.sort_values(['item_id'], ascending=False).head(2)\n",
    "\n",
    "train_df = pd.merge(train_df, enc, how='left', on='category_name')\n",
    "test_df = pd.merge(test_df, enc, how='left', on='category_name')\n",
    "\n",
    "train_df['days'].fillna(train_df['days_impute'], inplace=True)\n",
    "test_df['days'].fillna(test_df['days_impute'], inplace=True)\n",
    "\n",
    "\n",
    "#Create image flag \n",
    "test_df['image'] = test_df['image'].map(lambda x: 1 if len(str(x)) >0 else 0)\n",
    "train_df['image'] = train_df['image'].map(lambda x: 1 if len(str(x)) >0 else 0)\n",
    "\n",
    "# City names are duplicated across region, HT: Branden Murray \n",
    "#https://www.kaggle.com/c/avito-demand-prediction/discussion/55630#321751\n",
    "train_df['city'] = train_df['city'] + \"_\" + train_df['region']\n",
    "test_df['city'] = test_df['city'] + \"_\" + test_df['region']\n",
    "\n",
    "train_df['price'].fillna(0, inplace=True)\n",
    "test_df['price'].fillna(0, inplace=True)\n",
    "train_df['price'] = np.log1p(train_df['price'])\n",
    "test_df['price'] = np.log1p(test_df['price'])\n",
    "\n",
    "price_mean = train_df['price'].mean()\n",
    "price_std = train_df['price'].std()\n",
    "train_df['price'] = (train_df['price'] - price_mean) / price_std\n",
    "test_df['price'] = (test_df['price'] - price_mean) / price_std\n",
    "\n",
    "cat_cols = ['category_name', 'image_top_1']\n",
    "num_cols = ['price', 'deal_probability']\n",
    "\n",
    "for c in cat_cols:\n",
    "    for c2 in num_cols:\n",
    "        enc = train_df.groupby(c)[c2].agg(['median']).astype(np.float32).reset_index()\n",
    "        enc.columns = ['_'.join([str(c), str(c2), str(c3)]) if c3 != c else c for c3 in enc.columns]\n",
    "        train_df = pd.merge(train_df, enc, how='left', on=c)\n",
    "        test_df = pd.merge(test_df, enc, how='left', on=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "89082c16fd96cb7bf2ed68d74eaa022656724949",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TFIDF Vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,1))\n",
    "#ngram_range defines how you want to have words in your dictionary. \n",
    "#(min,max) = (1,2) will mean you will have unigrams and bigrms in your vocabulary. \n",
    "#Example String: \"The old fox\"\n",
    "#Vocabulary: \"The\", \"old\", \"fox\", \"The old\", \"old fox\"\n",
    "\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['title'].values.tolist() + test_df['title'].values.tolist())\n",
    "#train_df['title'].values.tolist() this converts all the values in the title column into a list. '+' appends two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "71277def8861c0ad7d25c12eaaa8fe3965857db2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tfidf = tfidf_vec.transform(train_df['title'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['title'].values.tolist())\n",
    "\n",
    "### SVD Components ###\n",
    "n_comp = 5\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "train_svd.columns = ['svd_title_'+str(i+1) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_title_'+str(i+1) for i in range(n_comp)]\n",
    "train_df = pd.concat([train_df, train_svd], axis=1)\n",
    "test_df = pd.concat([test_df, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b458b180ae81b26bb604830004925927cc1a2016",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,1), max_features=100000)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['description'].values.tolist() + test_df['description'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['description'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['description'].values.tolist())\n",
    "\n",
    "### SVD Components ###\n",
    "n_comp = 5\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "train_svd.columns = ['svd_desc_'+str(i+1) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_desc_'+str(i+1) for i in range(n_comp)]\n",
    "train_df = pd.concat([train_df, train_svd], axis=1)\n",
    "test_df = pd.concat([test_df, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "018bbff9eb2065c4406d96694af3addac6894180",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,1), max_features=100000)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['param123'].values.tolist() + test_df['param123'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['param123'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['param123'].values.tolist())\n",
    "\n",
    "### SVD Components ###\n",
    "n_comp = 5\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "train_svd.columns = ['svd_params_'+str(i+1) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_params_'+str(i+1) for i in range(n_comp)]\n",
    "train_df = pd.concat([train_df, train_svd], axis=1)\n",
    "test_df = pd.concat([test_df, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "44a4dfec9ac46ab8a366e77ad230b01e3e229927",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_df\n",
    "test = test_df\n",
    "\n",
    "# Label encode the categorical variables #\n",
    "cat_vars = [\"region\", \"city\", \"parent_category_name\", \"category_name\", \"user_type\", \"param_1\"]\n",
    "for col in cat_vars:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n",
    "    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n",
    "    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))\n",
    "\n",
    "cols_to_drop = [\"item_id\", \"user_id\", \"title\", \"description\", \"activation_date\", \"image\", \"param_2\", \"param_3\"\n",
    "                , \"param123\", \"image_top_1_impute\", \"days_impute\"]\n",
    "train_X = train_df.drop(cols_to_drop + [\"deal_probability\"], axis=1)\n",
    "test_X = test_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "train_y = train_df[\"deal_probability\"].values\n",
    "test_id = test_df[\"item_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "17ef5dc0db3ef85fdf2d906c7f4d46c4ca4ead49"
   },
   "outputs": [],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "77c1749da1b867a06db437ddb79ab59c598706a2"
   },
   "outputs": [],
   "source": [
    "#split the train into development and validation sample. Take the last 100K rows as validation sample.\n",
    "# Splitting the data for model training#\n",
    "dev_X = train_X.iloc[:-100000,:]\n",
    "val_X = train_X.iloc[-100000:,:]\n",
    "dev_y = train_y[:-100000]\n",
    "val_y = train_y[-100000:]\n",
    "print(dev_X.shape, val_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f6d6cddeb77e5f3571a6bc156d5d7fd885903e6f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#custom function to build the LightGBM model.\n",
    "def run_lgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"num_leaves\" : 1000,\n",
    "        \"learning_rate\" : 0.02,\n",
    "        \"bagging_fraction\" : 0.75,\n",
    "        \"feature_fraction\" : 0.6,\n",
    "        \"bagging_freq\" : 5,\n",
    "        \"bagging_seed\" : 2018,\n",
    "        \"verbosity\" : -1,\n",
    "        \"max_depth\": 18,\n",
    "        \"min_child_samples\":100\n",
    "       # ,\"boosting\":\"rf\"\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 2500, valid_sets=[lgval], early_stopping_rounds=50, verbose_eval=50, evals_result=evals_result)\n",
    "    \n",
    "    #model = lgb.cv(params, lgtrain, 1000, early_stopping_rounds=20, verbose_eval=20, stratified=False )\n",
    "    \n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model, evals_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "9b0862faf1afadc32ad6bb5d7d616b11ae32dc5d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the model #\n",
    "import lightgbm as lgb\n",
    "pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "a2751f8058a23655f15f081923db9b35307aecd3"
   },
   "outputs": [],
   "source": [
    "# Plot importance\n",
    "lgb.plot_importance(model, importance_type=\"split\", title=\"split\")\n",
    "plt.show()\n",
    "\n",
    "lgb.plot_importance(model, importance_type=\"gain\", title='gain')\n",
    "plt.show()\n",
    "\n",
    "# Importance values are also available in:\n",
    "print(model.feature_importance(\"split\"))\n",
    "print(model.feature_importance(\"gain\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "6d94d8d9e870608e8b2f8fbb0cb1a304b89c3be0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making a submission file #\n",
    "pred_test[pred_test>1] = 1\n",
    "pred_test[pred_test<0] = 0\n",
    "sub_df = pd.DataFrame({\"item_id\":test_id})\n",
    "sub_df[\"deal_probability\"] = pred_test\n",
    "sub_df.to_csv(\"baseline_lgb.csv\", index=False)\n",
    "\n",
    "! kaggle competitions submit -c avito-demand-prediction -f baseline_lgb.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "483dd45bc9560f728bc2e0f7a74759c04d3c9999",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(os.listdir(\"../working\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
